{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built with sktime 0.5.2\n",
    "# install conda environment file from environment.yml file in your command line: conda env create -f environment.yml\n",
    "# download sktime through: conda install -c conda-forge sktime\n",
    "# download and install latest sktime development version using the instructions on the file \"sktime installation from git.txt\"\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "from sktime.benchmarking.data import UEADataset, make_datasets\n",
    "from sktime.benchmarking.evaluation import Evaluator\n",
    "from sktime.benchmarking.metrics import PairwiseMetric, AggregateMetric\n",
    "from sktime.benchmarking.orchestration import Orchestrator\n",
    "from sktime.benchmarking.results import HDDResults\n",
    "from sktime.benchmarking.strategies import TSCStrategy\n",
    "from sktime.benchmarking.tasks import TSCTask\n",
    "from sktime.series_as_features.model_selection import PresplitFilesCV\n",
    "\n",
    "\n",
    "\n",
    "from sktime.classification.compose import (\n",
    "    ColumnEnsembleClassifier,\n",
    "    TimeSeriesForestClassifier,\n",
    ")\n",
    "\n",
    "from sktime.classification.dictionary_based import (\n",
    "    IndividualBOSS,\n",
    "    BOSSEnsemble,\n",
    "    ContractableBOSS,\n",
    "    TemporalDictionaryEnsemble,\n",
    "    IndividualTDE,\n",
    "    WEASEL,\n",
    "    MUSE,\n",
    ")\n",
    "\n",
    "from sktime.classification.shapelet_based import (\n",
    "    MrSEQLClassifier,\n",
    "    ShapeletTransformClassifier,\n",
    ")\n",
    "\n",
    "from sktime.classification.interval_based import TimeSeriesForest, RandomIntervalSpectralForest\n",
    "\n",
    "\n",
    "from sktime.classification.distance_based import (\n",
    "    ElasticEnsemble,\n",
    "    ProximityTree,\n",
    "    ProximityForest,\n",
    "    ProximityStump,\n",
    ")\n",
    "\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the multivaruate datasets from UEA which are related to medical domain\n",
    "# AtrialFibrillation, EigenWorms, Epilepsy, FingerMovements, HandMovementDirection, Heartbeat, \n",
    "# MotorImagery, StandWalkJump, FaceDetection, BasicMotions, ERing, SelfRegulationSCP1, SelfRegulationSCP2\n",
    "# Non ts file datasets: EyesOpenShut\n",
    "\n",
    "\n",
    "import os\n",
    "import sktime\n",
    "from sktime.utils.data_io import load_from_tsfile_to_dataframe, load_from_arff_to_dataframe\n",
    "\n",
    "# dowload http://www.timeseriesclassification.com/Downloads/Archives/Multivariate2018_ts.zip and extract to your desired path\n",
    "# change data path to the path where Multivariate_ts folder exists\n",
    "DATA_PATH = os.path.join(os.path.dirname(\"C:\\\\Users\\\\rbabayev\\\\Desktop\\\\\"), \"Multivariate_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_multivariate(X):\n",
    "    import pandas\n",
    "    if type(X) == pandas.core.frame.DataFrame and len(X.shape) == 2:\n",
    "        return X.shape[1] > 1\n",
    "    else:\n",
    "        return false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should run test for each dataset one by one by uncommenting them\n",
    "\n",
    "# (137, 3) (137,) (138, 3) (138,), length of the series is 207\n",
    "X_train, y_train = load_from_tsfile_to_dataframe(\n",
    "    os.path.join(DATA_PATH, \"Epilepsy/Epilepsy_TRAIN.ts\")\n",
    ")\n",
    "X_test, y_test = load_from_tsfile_to_dataframe(\n",
    "    os.path.join(DATA_PATH, \"Epilepsy/Epilepsy_TEST.ts\")\n",
    ")\n",
    "\n",
    "\n",
    "# # (316, 28) (316,) (100, 28) (100,), length of the series is 50\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"FingerMovements/FingerMovements_TRAIN.ts\")\n",
    "# )\n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"FingerMovements/FingerMovements_TEST.ts\")\n",
    "# )\n",
    "\n",
    "\n",
    "# # (160, 10) (160,) (74, 10) (74,), length of the series is 400\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"HandMovementDirection/HandMovementDirection_TRAIN.ts\")\n",
    "# )\n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"HandMovementDirection/HandMovementDirection_TEST.ts\")\n",
    "# )\n",
    "\n",
    "\n",
    "# # (204, 61) (204,) (205, 61) (205,),  length of series is 405\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"Heartbeat/Heartbeat_TRAIN.ts\")\n",
    "# )\n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"Heartbeat/Heartbeat_TEST.ts\")\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # (40, 6) (40,) (40, 6) (40,), length of the series is 100\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"BasicMotions/BasicMotions_TRAIN.ts\")\n",
    "# )\n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"BasicMotions/BasicMotions_TEST.ts\")\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # (268, 6) (268,) (293, 6) (293,), length of the series is 896\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"SelfRegulationSCP1/SelfRegulationSCP1_TRAIN.ts\")\n",
    "# )\n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"SelfRegulationSCP1/SelfRegulationSCP1_TEST.ts\")\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # (200, 7) (200,) (180, 7) (180,), length of the series is 1152\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"SelfRegulationSCP2/SelfRegulationSCP2_TRAIN.ts\")\n",
    "# )\n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\n",
    "#     os.path.join(DATA_PATH, \"SelfRegulationSCP2/SelfRegulationSCP2_TEST.ts\")\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# download http://www.timeseriesclassification.com/Downloads/EyesOpenShut.zip \n",
    "# and extract EyesOpenShut_TRAIN.arff and EyesOpenShut_TEST.arff to your desired path\n",
    "# # path for multivariate datasets which are not available in the UEA and UCR zip files\n",
    "# o_path = os.path.join(os.path.dirname(\"C:\\\\Users\\\\rbabayev\\\\Desktop\\\\\"), \"Other_datasets_arff\\\\multivariate\")\n",
    "\n",
    "\n",
    "# # (56, 14) (56,) (42, 14) (42,), length of the time series is 128\n",
    "# X_train, y_train = load_from_arff_to_dataframe(\n",
    "#     os.path.join(o_path, \"EyesOpenShut/EyesOpenShut_TRAIN.arff\")\n",
    "# )\n",
    "# X_test, y_test = load_from_arff_to_dataframe(\n",
    "#     os.path.join(o_path, \"EyesOpenShut/EyesOpenShut_TEST.arff\")\n",
    "# )\n",
    "\n",
    "\n",
    "print(\"Multivariate dataset -> \", is_multivariate(X_train) and is_multivariate(X_test))\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-class target variable\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize before applying PCA\n",
    "# PCA performs best with a standardized feature set. We will perform standard scalar normalization to normalize our feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils.data_processing import (\n",
    "    from_3d_numpy_to_2d_array,\n",
    "    from_3d_numpy_to_nested,\n",
    "    from_nested_to_2d_array,\n",
    "    from_2d_array_to_nested,\n",
    "    from_nested_to_3d_numpy,\n",
    "    from_nested_to_long,\n",
    "    #from_long_to_nested,\n",
    "    #from_multi_index_to_3d_numpy,\n",
    "    #from_3d_numpy_to_multi_index,\n",
    "    #from_multi_index_to_nested,\n",
    "    #from_nested_to_multi_index,\n",
    "    #are_columns_nested,\n",
    "    is_nested_dataframe,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_transform(X):    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sktime.transformations.panel.compose import SeriesToSeriesRowTransformer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    t = StandardScaler(with_mean=True, with_std=True)\n",
    "    r = SeriesToSeriesRowTransformer(t, check_transformer=False)\n",
    "\n",
    "    Xt = r.fit_transform(X)\n",
    "    assert Xt.shape == X.shape\n",
    "    assert isinstance(\n",
    "        Xt.iloc[0, 0], (pd.Series, np.ndarray)\n",
    "    )  # check series-to-series transform\n",
    "    np.testing.assert_almost_equal(Xt.iloc[0, 0].mean(), 0)  # check standardisation\n",
    "    np.testing.assert_almost_equal(Xt.iloc[0, 0].std(), 1, decimal=2)\n",
    "    \n",
    "    # rename volumns\n",
    "    Xt.columns = X.columns.tolist()\n",
    "    \n",
    "    return (Xt, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length = from_nested_to_3d_numpy(X_train).shape[2]\n",
    "\n",
    "print(\"len(X_train) => \", len(X_train))\n",
    "print(\"len(X_test) => \", len(X_test))\n",
    "print(\"Initial ts_length => \", ts_length)\n",
    "\n",
    "X_train_t, r = standardize_transform(X_train)\n",
    "#X_test_t = standardize_transform(X_test)\n",
    "X_test_t = r.transform(X_test)\n",
    "X_test_t.columns = X_test.columns.tolist()\n",
    "\n",
    "\n",
    "#X_train = X_train_t\n",
    "#X_test = X_test_t\n",
    "\n",
    "ts_length_train = from_nested_to_3d_numpy(X_train_t).shape[2]\n",
    "ts_length_test = from_nested_to_3d_numpy(X_test_t).shape[2]\n",
    "\n",
    "print(\"Standardized len(X_train) => \", len(X_train_t))\n",
    "print(\"Standardized len(X_test) => \", len(X_test_t))\n",
    "print(\"Standardized ts_length X_train_t => \", ts_length_train)\n",
    "print(\"Standardized ts_length X_test_t => \", ts_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"is_nested_dataframe: \", is_nested_dataframe(X_train_t) and is_nested_dataframe(X_test_t))\n",
    "print(\"num_inst, num_dims, ts_length: \", from_nested_to_3d_numpy(X_train_t).shape, from_nested_to_3d_numpy(X_test_t).shape)\n",
    "\n",
    "X_train_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform_mts(X, y=None, n_components=None):\n",
    "    from sktime.transformations.panel.pca import PCATransformer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sys\n",
    "    \n",
    "    # if n_components == 'mle' Minkaâ€™s MLE is used to guess the dimension\n",
    "    # If 0 < n_components < 1, select the number of components such that the amount of variance \n",
    "    # that needs to be explained is greater than the percentage specified by n_components.\n",
    "#     pca = PCATransformer(n_components = n_components, random_state=1, \n",
    "#                          #whiten=True, \n",
    "#                          #svd_solver=\"arpack\", \n",
    "#                          #tol=10.0\n",
    "#                          svd_solver=\"full\",\n",
    "                         \n",
    "#                         )\n",
    "\n",
    "    cols = X.columns.tolist()\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    fitted_pcas = []\n",
    "\n",
    "    for i, col in enumerate(X):\n",
    "        pca = PCATransformer(n_components = n_components, random_state=1, \n",
    "                         #whiten=True, \n",
    "                         #svd_solver=\"arpack\", \n",
    "                         #tol=10.0\n",
    "                         svd_solver=\"full\",\n",
    "                        )\n",
    "        dim_i_df = pd.DataFrame(X[col])\n",
    "        #print(dim_i_df.shape)\n",
    "        #print(dim_i_df.columns)\n",
    "        #print(len(dim_i_df['dim_' + str(i)][0]))\n",
    "        pca.fit(dim_i_df, y)\n",
    "        dim_i_df = pca.transform(dim_i_df)\n",
    "        #dim_i_df.columns = ['dim_' + str(i),]\n",
    "        dim_i_df.columns = ['dim_0',]\n",
    "        #print(len(dim_i_df['dim_' + str(i)]))\n",
    "        #print(len(dim_i_df['dim_' + str(i)][0]))\n",
    "        #print(len(dim_i_df['dim_0'][0]))\n",
    "        frames.append(dim_i_df)\n",
    "        fitted_pcas.append(pca)\n",
    "    \n",
    "    \n",
    "#     # sanity check\n",
    "#     lens = []\n",
    "#     for i, dim_df in enumerate(frames):\n",
    "#         for j, col in enumerate(dim_df):\n",
    "#             for k in range(len(dim_df[col])):\n",
    "#                 #print(\"dim_df_\" + str(i) + \"['\" + \"dim_\" + str(j) + \"']\" + \"[\" + str(k) +\"]\", \" => \", len(dim_df[col][k]))\n",
    "#                 lens.append(len(dim_df[col][k]))\n",
    "                \n",
    "#     if(len(np.unique(lens)) > 1):\n",
    "#         #assert Error\n",
    "#         print(\"Sanity check failed!\", file=sys.stderr)\n",
    "    \n",
    "    #column concat dataframes\n",
    "    X_tr = pd.concat(frames, axis=1, ignore_index=True)\n",
    "    # rename columns\n",
    "    X_tr.columns = cols\n",
    "    \n",
    "    return (X_tr, fitted_pcas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_with_fitted_pcas(X, fitted_pcas):\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    \n",
    "    cols = X.columns.tolist()\n",
    "    \n",
    "    frames = []\n",
    "\n",
    "    for i, col in enumerate(X):\n",
    "        dim_i_df = pd.DataFrame(X[col])\n",
    "        dim_i_df = fitted_pcas[i].transform(dim_i_df)\n",
    "        dim_i_df.columns = ['dim_0',]\n",
    "        frames.append(dim_i_df)\n",
    "    \n",
    "    \n",
    "#     # sanity check\n",
    "#     lens = []\n",
    "#     for i, dim_df in enumerate(frames):\n",
    "#         for j, col in enumerate(dim_df):\n",
    "#             for k in range(len(dim_df[col])):\n",
    "#                 #print(\"dim_df_\" + str(i) + \"['\" + \"dim_\" + str(j) + \"']\" + \"[\" + str(k) +\"]\", \" => \", len(dim_df[col][k]))\n",
    "#                 lens.append(len(dim_df[col][k]))\n",
    "                \n",
    "#     if(len(np.unique(lens)) > 1):\n",
    "#         #assert Error\n",
    "#         print(\"Sanity check failed!\", file=sys.stderr)\n",
    "    \n",
    "    #column concat dataframes\n",
    "    X_tr = pd.concat(frames, axis=1, ignore_index=True)\n",
    "    # rename columns\n",
    "    X_tr.columns = cols\n",
    "    \n",
    "    return X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.panel.pca import PCATransformer\n",
    "import pandas as pd\n",
    "\n",
    "#X_train_t = X_train\n",
    "#X_test_t = X_test\n",
    "\n",
    "#print(len(X_train_t['dim_0'][0]))\n",
    "#print(len(X_test_t['dim_0'][0]))\n",
    "\n",
    "ts_length_train = from_nested_to_3d_numpy(X_train_t).shape[2]\n",
    "ts_length_test = from_nested_to_3d_numpy(X_train_t).shape[2]\n",
    "\n",
    "print(\"len(X_train) => \", len(X_train_t))\n",
    "print(\"len(X_test) => \", len(X_test_t))\n",
    "print(\"Initial ts_length X_train => \", ts_length_train)\n",
    "print(\"Initial ts_length X_test => \", ts_length_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DEFINE THRESHOLDS FOR Epilepsy, FingerMovements, HandMovementDirection, Heartbeat, BasicMotions, SelfRegulationSCP1, \n",
    "# SelfRegulationSCP2 AS 35, 20, 25, 35, 25, 20, 25, 25 RESPECTIVELY\n",
    "threshold = 25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# UNCOMMENT THRESHOLD BELOW AFTER GIVING IT A RIGHT VALUE\n",
    "# if length of the series is smaller than len(X_train), len(X_test) then it will be real number of components\n",
    "n_components_initial = min(len(X_train_t), len(X_test_t))\n",
    "#n_components_initial = len(X_train_t) + len(X_test_t)\n",
    "n_components_initial = min(n_components_initial, ts_length_train, ts_length_test,\n",
    "                          #threshold\n",
    "                          )\n",
    "#n_components_initial = n_components_initial - 1\n",
    "\n",
    "\n",
    "# n_components='mle' is only supported if n_samples >= n_features\n",
    "X_train_ts, fitted_pcas = pca_transform_mts(X_train_t, y_train, n_components_initial)\n",
    "#X_test_ts, fitted_pcass = pca_transform_mts(X_test_t, y_test, n_components_initial)\n",
    "cols = [pca.pca.explained_variance_ratio_ for pca in fitted_pcas]\n",
    "X_test_ts = transform_with_fitted_pcas(X_test_t, fitted_pcas)\n",
    "\n",
    "\n",
    "# # \"mle\" generates different length for each dimension\n",
    "# big_ts = pca_transform_mts(pd.concat([X_train_t, X_test_t], ignore_index=True), n_components=n_components_initial)\n",
    "# X_train_ts = big_ts[:len(X_train_t)]\n",
    "# X_test_ts = big_ts[len(X_train_t):len(big_ts)]\n",
    "# # inplace reset index to start from 0\n",
    "# X_test_ts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"X_train_ts size => \", len(X_train_ts))\n",
    "print(\"X_test_ts size => \", len(X_test_ts))\n",
    "\n",
    "\n",
    "print(\"ts_length of X_train_ts => \", from_nested_to_3d_numpy(X_train_ts).shape[2])\n",
    "print(\"ts_length of X_test_ts => \", from_nested_to_3d_numpy(X_train_ts).shape[2])\n",
    "\n",
    "\n",
    "print(\"Multivariate dataset -> \", is_multivariate(X_train_ts) and is_multivariate(X_test_ts))\n",
    "print(X_train_ts.shape, y_train.shape, X_test_ts.shape, y_test.shape)\n",
    "print(X_train_ts.shape)\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "#style layout \n",
    "layout = go.Layout(\n",
    "    #title=\"Title\",\n",
    "    xaxis=dict(\n",
    "        title=\"Number of principal components\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Variance [%]\"\n",
    "    ) ) \n",
    "fig = go.Figure(layout=layout)\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    indices = list(range(1, len(col) + 1))\n",
    "    fig.add_trace(go.Scatter(\n",
    "                x=indices, y=[e * 100 for e in col], mode=\"lines+markers\",\n",
    "                name = \"dim \" + str(i + 1),\n",
    "                showlegend=False,\n",
    "            ))\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "#                 x=[threshold] * len(cols[0]), y=[e * 100 for e in cols[0]], mode=\"lines\", line={'dash': 'dash', 'color': 'black'},\n",
    "#                 name=\"threshold\"\n",
    "#             ))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "                x=indices, y=[1] * len(cols[0]), mode=\"lines\", line={'dash': 'dash', 'color': 'black'},\n",
    "                name=\"threshold\",\n",
    "                showlegend=False,\n",
    "            ))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-class target variable\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column ensembling\n",
    "# We can also fit one classifier for each time series column and then aggregated their predictions. \n",
    "# The interface is similar to the familiar ColumnTransformer from sklearn.\n",
    "\n",
    "clf_list = [\n",
    "#     ColumnEnsembleClassifier(\n",
    "#     estimators=[\n",
    "#         (\"TSFC0\", TimeSeriesForestClassifier(n_estimators=100, random_state=1), [0]),\n",
    "#     ]\n",
    "# ),\n",
    "    \n",
    "    ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"TSF0\", TimeSeriesForest(n_estimators=100, random_state=1), [0]),\n",
    "    ]\n",
    "),\n",
    "    \n",
    "    ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"RandomIntervalSpectralForest0\", RandomIntervalSpectralForest(n_estimators=100, random_state=1), [0]),\n",
    "    ]\n",
    "),\n",
    "    \n",
    "#     ColumnEnsembleClassifier(\n",
    "#     estimators=[\n",
    "#         (\"BOSSEnsemble0\", BOSSEnsemble(max_ensemble_size=5, random_state=1), [0]),\n",
    "#     ]\n",
    "# ),\n",
    "#     ColumnEnsembleClassifier(\n",
    "#     estimators=[\n",
    "#         (\"TemporalDictionaryEnsemble0\", TemporalDictionaryEnsemble(n_parameter_samples=250, max_ensemble_size=50,\n",
    "#                                                                    randomly_selected_params=50, random_state=1), [0])\n",
    "#     ]\n",
    "# ),\n",
    "   ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"KNeighborsTimeSeriesClassifier0\", KNeighborsTimeSeriesClassifier(n_neighbors=1, metric=\"dtw\"), [0])\n",
    "    ]\n",
    "),  \n",
    "    \n",
    "    ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"ContractableBOSS0\", ContractableBOSS(n_parameter_samples=250, max_ensemble_size=50, random_state=1), [0])\n",
    "    ]\n",
    "),  \n",
    "    \n",
    "#     ColumnEnsembleClassifier(\n",
    "#     estimators=[\n",
    "#         (\"CanonicalIntervalForest0\", CanonicalIntervalForest(n_estimators=100, att_subsample_size=8, random_state=1), [0])\n",
    "#     ]\n",
    "# ),  \n",
    "    \n",
    "    \n",
    "#     ColumnEnsembleClassifier(\n",
    "#     estimators=[\n",
    "#         (\"STC0\", ShapeletTransformClassifier(time_contract_in_mins=1, random_state=1), [0])\n",
    "#     ]\n",
    "# ),  \n",
    "    \n",
    "    ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"WSL0\", WEASEL(binning_strategy=\"equi-depth\", anova=False, random_state=1), [0])\n",
    "    ]\n",
    "),  \n",
    "    \n",
    "#     ColumnEnsembleClassifier(\n",
    "#     estimators=[\n",
    "#         (\"EE0\", ElasticEnsemble(random_state=1), [0])\n",
    "#     ]\n",
    "# ),  \n",
    "    \n",
    "#      ColumnEnsembleClassifier(\n",
    "#     estimators=[\n",
    "#         (\"PF0\", ProximityForest(n_estimators=100, random_state=1), [0])\n",
    "#     ]\n",
    "# ),  \n",
    "    \n",
    "    MrSEQLClassifier(),\n",
    "    MUSE(random_state=1),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# for clf in clf_list:\n",
    "#     print(\"\\n-------------------------------------------\")\n",
    "#     print(clf)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     y_test_prob = clf.predict_proba(X_test)\n",
    "#     y_test_pred = clf.predict(X_test) \n",
    "#     print(\"accuracy: \", accuracy_score(y_test, y_test_pred))\n",
    "#     print(\"f1 score: \", f1_score(y_test, y_test_pred, average='macro'))\n",
    "    \n",
    "#     if len(np.unique(y_train)) == 2:\n",
    "#         # for binary classification make_scorer should be used: https://github.com/scikit-learn/scikit-learn/issues/10247\n",
    "#         print(\"auroc: \", make_scorer(roc_auc_score, needs_proba=True)(clf, X_test, y_test))\n",
    "#     else:\n",
    "#         print(\"auroc: \", roc_auc_score(y_test, y_test_prob, average='macro', multi_class=\"ovo\"))\n",
    "        \n",
    "#     #print(\"auprc: \", average_precision_score(y_test, y_test_prob)) # multiclass format is not supported\n",
    "#     #print(\"auprc: \", make_scorer(average_precision_score, needs_proba=True)(clf, X_test, y_test)) # multiclass format is not supported\n",
    "#     print(\"recall: \", recall_score(y_test, y_test_pred, average='macro'))\n",
    "#     #break\n",
    "    \n",
    "\n",
    "\n",
    "for clf in clf_list:\n",
    "    print(\"\\n-------------------------------------------\")\n",
    "    print(clf)\n",
    "    clf.fit(X_train_ts, y_train)\n",
    "    y_test_prob = clf.predict_proba(X_test_ts)\n",
    "    y_test_pred = clf.predict(X_test_ts) \n",
    "    print(\"accuracy: \", accuracy_score(y_test, y_test_pred))\n",
    "    print(\"f1 score: \", f1_score(y_test, y_test_pred, average='macro'))\n",
    "    \n",
    "    if len(np.unique(y_train)) == 2:\n",
    "        # for binary classification make_scorer should be used: https://github.com/scikit-learn/scikit-learn/issues/10247\n",
    "        print(\"auroc: \", make_scorer(roc_auc_score, needs_proba=True)(clf, X_test_ts, y_test))\n",
    "    else:\n",
    "        print(\"auroc: \", roc_auc_score(y_test, y_test_prob, average='macro', multi_class=\"ovo\"))\n",
    "        \n",
    "    #print(\"auprc: \", average_precision_score(y_test, y_test_prob)) # multiclass format is not supported\n",
    "    #print(\"auprc: \", make_scorer(average_precision_score, needs_proba=True)(clf, X_test, y_test)) # multiclass format is not supported\n",
    "    print(\"recall: \", recall_score(y_test, y_test_pred, average='macro'))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
